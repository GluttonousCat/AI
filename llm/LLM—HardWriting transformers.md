# Transformer:hugs:

## Position Embedding

```python
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class PositionEmbedding(nn.module):
    """
    PositionEmbedidng
    Attribute:
        
    """
    def __init__(self):
        super().__init__()
        pass
    
    def forward(self):
        pass
```



## Multi Head Attention

```python
class MultiHeadAttention(nn.module):
    def __init__(self):
        super().__init__()
        pass
    
    def forward(self):
        pass
```

## Feed Forward Network

```python
class FeedForward(nn.module):
    def __init__(self):
        super().__init__()
        pass
    
    def forward(self):
        pass
    
```

## Encoder Layer

```python
class EncoderLayer(nn.module):
    def __init__(self):
        super().__init__()
        pass
    
    def forward(self):
        pass
    
```



## Transformer

```python
class Transformers(nn.module):
    def __init__(self):
        super().__init__()
        pass
    
    def forward(self):
        pass
```

